#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ IHDP notebook
"""

import numpy as np
from tqdm import tqdm

from tensoraerospace.envs.f16.linear_longitudial import LinearLongitudinalF16
from tensoraerospace.utils import generate_time_period, convert_tp_to_sec_tp
from tensoraerospace.signals.standart import unit_step
from tensoraerospace.agent.ihdp.model import IHDPAgent
from tensoraerospace.benchmark import ControlBenchmark
import gymnasium as gym

def main():
    print("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ IHDP notebook...")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    dt = 0.01  # –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è
    tp = generate_time_period(tn=20, dt=dt) # –í—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥
    tps = convert_tp_to_sec_tp(tp, dt=dt)
    number_time_steps = len(tp) # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
    reference_signals = np.reshape(unit_step(degree=5, tp=tp, time_step=10, output_rad=True), [1, -1]) # –ó–∞–¥–∞–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
    initial_state = [[0], [0]]  # –î–ª—è theta, alpha
    
    env = gym.make('LinearLongitudinalF16-v0', 
                   number_time_steps=number_time_steps, 
                   initial_state=initial_state,  # 2 —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–ª—è 2 —Å–æ—Å—Ç–æ—è–Ω–∏–π
                   reference_signal=reference_signals, 
                   state_space=["theta", "alpha"],
                   output_space=["theta", "alpha"], 
                   control_space=["ele"],    
                   tracking_states=["alpha"])
    env.reset()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≥–µ–Ω—Ç–∞
    actor_settings = {
        "start_training": 5,
        "layers": (25, 1), 
        "activations":  ('tanh', 'tanh'), 
        "learning_rate": 2, 
        "learning_rate_exponent_limit": 10,
        "type_PE": "combined",
        "amplitude_3211": 15, 
        "pulse_length_3211": 5/dt, 
        "maximum_input": 25,
        "maximum_q_rate": 20,
        "WB_limits": 30,
        "NN_initial": 120,
        "cascade_actor": False,
        "learning_rate_cascaded":1.2
    }
    
    incremental_settings = {
        "number_time_steps": number_time_steps, 
        "dt": dt, 
        "input_magnitude_limits":25, 
        "input_rate_limits":60,
    }
    
    critic_settings = {
        "Q_weights": [8], 
        "start_training": -1, 
        "gamma": 0.99, 
        "learning_rate": 15, 
        "learning_rate_exponent_limit": 10,
        "layers": (25,1), 
        "activations": ("tanh", "linear"), 
        "WB_limits": 30,
        "NN_initial": 120,
        "indices_tracking_states": env.unwrapped.indices_tracking_states
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = IHDPAgent(actor_settings, critic_settings, incremental_settings, 
                      env.unwrapped.tracking_states, env.unwrapped.state_space, 
                      env.unwrapped.control_space, number_time_steps, 
                      env.unwrapped.indices_tracking_states)
    
    # –ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–∏
    xt = np.array([[0], [0]])
    
    print("–ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–∏ IHDP –∞–≥–µ–Ω—Ç–∞...")
    
    for step in tqdm(range(number_time_steps-3)):
        ut = model.predict(xt, reference_signals, step)
        xt, reward, terminated, truncated, info = env.step(np.array(ut))
    
    print(f"–°–∏–º—É–ª—è—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—ã–ø–æ–ª–Ω–µ–Ω–æ {step+1} —à–∞–≥–æ–≤.")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n=== –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Å–∏–º—É–ª—è—Ü–∏–∏: {env.unwrapped.model.time_step}")
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –º–æ–¥–µ–ª–∏
    states_data = env.unwrapped.model.store_states[:, :env.unwrapped.model.time_step].T
    control_data = env.unwrapped.model.store_input[:, :env.unwrapped.model.time_step].T
    
    print(f"–§–æ—Ä–º–∞ –º–∞—Å—Å–∏–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π: {states_data.shape}")
    print(f"–§–æ—Ä–º–∞ –º–∞—Å—Å–∏–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {control_data.shape}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è alpha
    alpha_history = states_data[:, 1]  # alpha - –≤—Ç–æ—Ä–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    alpha_ref = reference_signals[0, :env.unwrapped.model.time_step]
    alpha_error = np.abs(alpha_history - alpha_ref)
    
    print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è alpha:")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.mean(alpha_error):.6f} —Ä–∞–¥")
    print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.max(alpha_error):.6f} —Ä–∞–¥")
    print(f"  –§–∏–Ω–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ alpha: {alpha_history[-1]:.6f} —Ä–∞–¥")
    print(f"  –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ alpha: {alpha_ref[-1]:.6f} —Ä–∞–¥")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
    final_error = np.abs(alpha_history[-1] - alpha_ref[-1])
    if final_error < 0.1:
        print(f"‚úÖ –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ {final_error:.6f} < 0.1")
        print("\n============================================================")
        print("üéâ NOTEBOOK –ò–°–ü–†–ê–í–õ–ï–ù –ò –†–ê–ë–û–¢–ê–ï–¢ –ö–û–†–†–ï–ö–¢–ù–û!")
        print("IHDP –∞–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∑–∞–¥–∞–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª.")
        return True
    else:
        print(f"‚ùå –¢–ï–°–¢ –ù–ï –ü–†–û–ô–î–ï–ù: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ {final_error:.6f} >= 0.1")
        return False

if __name__ == "__main__":
    main()