{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensoraerospace.agent.narx.model import NARX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from tensoraerospace.envs.f16.linear_longitudial import LinearLongitudinalF16\n",
    "from tensoraerospace.utils import generate_time_period, convert_tp_to_sec_tp\n",
    "from tensoraerospace.signals.standart import unit_step, sinusoid\n",
    "from tensoraerospace.benchmark.function import overshoot, settling_time, static_error\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01  # Дискретизация\n",
    "tp = generate_time_period(tn=20, dt=dt) # Временной периуд\n",
    "tps = convert_tp_to_sec_tp(tp, dt=dt)\n",
    "number_time_steps = len(tp) # Количество временных шагов\n",
    "reference_signals = np.reshape(sinusoid(amplitude=5, tp=tp, frequency=5.0), [1, -1]) # Заданный сигнал\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(tps, sinusoid(amplitude=0.01, tp=tp, frequency=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация списка для хранения исторических данных\n",
    "hist = []\n",
    "dt = 0.01  # Интервал дискретизации времени\n",
    "\n",
    "# Генерация временного периода с заданным интервалом дискретизации\n",
    "tp = generate_time_period(tn=40, dt=dt) \n",
    "\n",
    "# Конвертация временного периода в секунды\n",
    "tps = convert_tp_to_sec_tp(tp, dt=dt)\n",
    "\n",
    "# Вычисление общего количества временных шагов\n",
    "number_time_steps = len(tp) \n",
    "\n",
    "# Создание заданного сигнала с использованием единичного шага\n",
    "# reference_signals = np.reshape(unit_step(degree=0, tp=tp, time_step=20, output_rad=True), [1, -1])\n",
    "reference_signals = np.reshape(np.deg2rad(sinusoid(amplitude=0.01, tp=tp, frequency=5)), [1, -1])\n",
    "\n",
    "# Создание среды симуляции, задание временных шагов, начального состояния, заданного сигнала и отслеживаемых состояний\n",
    "env = gym.make('LinearLongitudinalF16-v0',\n",
    "               number_time_steps=number_time_steps, \n",
    "               initial_state=[[0],[0],[0],[0]],\n",
    "               reference_signal=reference_signals,\n",
    "               tracking_states=[\"alpha\"])\n",
    "\n",
    "# Сброс среды к начальному состоянию\n",
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.low, env.action_space.high "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensoraerospace.agent.a2c.narx import Actor, Mish, Critic, A2CLearner, Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, n_actions, activation=Mish)\n",
    "critic = Critic(state_dim, activation=Mish)\n",
    "\n",
    "learner = A2CLearner(actor, critic, entropy_beta=0.6)\n",
    "runner = Runner(env, actor, writer=learner.writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_on_memory = 100\n",
    "episodes = 50000\n",
    "episode_length = 4000\n",
    "total_steps = (episode_length*episodes)//steps_on_memory\n",
    "\n",
    "for i in tqdm(range(total_steps)):\n",
    "    memory = runner.run(steps_on_memory)\n",
    "    learner.learn(memory, runner.steps, discount_rewards=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, reward, state, next_state, done = memory[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Загрузка сохраненных весов\n",
    "# actor.load_state_dict(torch.load('best_actor.pth'))\n",
    "# critic.load_state_dict(torch.load('best_critic.pth'))\n",
    "# env = gym.make('LinearLongitudinalF16-v0',\n",
    "#                number_time_steps=number_time_steps, \n",
    "#                initial_state=[[0],[0],[0],[0]],\n",
    "#                reference_signal=reference_signals,\n",
    "#                tracking_states=[\"alpha\"])\n",
    "\n",
    "# Демонстрация обученного агента\n",
    "num_demo_episodes = 5\n",
    "\n",
    "for episode in range(num_demo_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    frames = 0\n",
    "    prev_action = np.zeros(env.action_space.shape)\n",
    "    while not done:\n",
    "        dists = actor(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "        actions = dists.sample().detach().data.numpy()\n",
    "        actions_clipped = np.clip(actions, env.action_space.low.min(), env.action_space.high.max())\n",
    "        next_state, reward, terminated, truncated, info= env.step(actions_clipped[0])\n",
    "        prev_action = actions_clipped[0]  # Update the previous action\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        frames +=1\n",
    "    print(f\"Demo Episode {episode}, Total Reward: {total_reward}, {frames}\")\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.model.plot_control('ele', tps, to_deg=True, figsize=(15,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.model.plot_transient_process('alpha', tps, reference_signals[0], to_deg=True, figsize=(15,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
