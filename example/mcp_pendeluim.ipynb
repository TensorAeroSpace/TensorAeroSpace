{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/asmazaev/Projects/TensorAeroSpace\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Uniform\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Создает нейронную сеть для моделирования динамики системы.\n",
    "\n",
    "    Сеть состоит из трех линейных слоев и функций активации ReLU между ними.\n",
    "    Входной слой принимает вектор из 3 элементов, представляющих состояния системы.\n",
    "    Второй и третий слои - это скрытые слои с 128 нейронами.\n",
    "    Выходной слой генерирует вектор из 2 элементов, представляющих предсказание следующего состояния системы.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 128)  # 3 состояния + 1 действие = 4\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)  # Предсказание следующего состояния\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Выполняет прямое распространение входных данных через сеть.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Входные данные, представляющие состояния системы.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Предсказание следующего состояния системы.\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MPCAgent(object):\n",
    "    \"\"\"\n",
    "    Агент, использующий метод Модельно-Прогностического Управления (MPC) для оптимизации действий в среде.\n",
    "\n",
    "    Attributes:\n",
    "        gamma (float): Коэффициент дисконтирования.\n",
    "        action_dim (int): Размерность пространства действий.\n",
    "        observation_dim (int): Размерность пространства наблюдений.\n",
    "        model (torch.nn.Module): Модель для аппроксимации динамики среды.\n",
    "        cost_function (callable): Функция стоимости, используемая для оценки действий.\n",
    "        lr (float): Скорость обучения для оптимизатора модели.\n",
    "        criterion (torch.nn.modules.loss): Критерий потерь для обучения модели.\n",
    "\n",
    "    Methods:\n",
    "        train_model(states, actions, next_states, epochs=100, batch_size=64):\n",
    "            Обучает модель динамики среды.\n",
    "        collect_data(env, num_episodes=1000):\n",
    "            Собирает данные о состояниях, действиях и следующих состояниях, исполняя политику в среде.\n",
    "        choose_action(state, rollout, horizon):\n",
    "            Выбирает оптимальное действие, используя прогнозируемое моделирование.\n",
    "        choose_action_ref(state, rollout, horizon, reference_signals, step):\n",
    "            Выбирает оптимальное действие с учетом эталонных сигналов.\n",
    "        test_model(env, num_episodes=100, rollout=10, horizon=1):\n",
    "            Тестирует модель, измеряя среднее вознаграждение в среде.\n",
    "        test_network(states, actions, next_states):\n",
    "            Тестирует точность предсказаний модели на заданном наборе данных.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, action_dim, observation_dim, model, cost_function, lr=1e-3, criterion=torch.nn.MSELoss()):\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "        self.observation_dim = observation_dim\n",
    "        self.system_model = model\n",
    "        self.system_model_optimizer = optim.Adam(self.system_model.parameters(), lr=lr)\n",
    "        self.cost_function = cost_function\n",
    "        self.writer = SummaryWriter()\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def train_model(self, states, actions, next_states, epochs=100, batch_size=64):\n",
    "        \"\"\"\n",
    "        Обучает модель динамики среды, используя данные о состояниях, действиях и следующих состояниях.\n",
    "\n",
    "        Args:\n",
    "            states (numpy.ndarray): Массив текущих состояний.\n",
    "            actions (numpy.ndarray): Массив действий, совершенных в этих состояниях.\n",
    "            next_states (numpy.ndarray): Массив следующих состояний после совершения действий.\n",
    "            epochs (int): Количество эпох обучения.\n",
    "            batch_size (int): Размер батча для обучения.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for epoch in  (pbar := tqdm(range(epochs))):\n",
    "            permutation = np.random.permutation(states.shape[0])\n",
    "            for i in range(0, states.shape[0], batch_size):\n",
    "                indices = permutation[i:i+batch_size]\n",
    "                batch_states, batch_actions, batch_next_states = states[indices], actions[indices], next_states[indices]\n",
    "                inputs = np.hstack((batch_states, batch_actions.reshape(-1, 1)))\n",
    "                inputs = torch.tensor(inputs, dtype=torch.float32).to(\"mps\")\n",
    "                targets = torch.tensor(batch_next_states, dtype=torch.float32).to(\"mps\")\n",
    "                self.system_model_optimizer.zero_grad()\n",
    "                outputs = self.system_model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                self.system_model_optimizer.step()\n",
    "            \n",
    "            self.writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "            pbar.set_description(f\"Loss {loss.item()}\")\n",
    "\n",
    "    def collect_data(self, env, num_episodes=1000):\n",
    "        \"\"\"\n",
    "        Собирает данные о состояниях, действиях и следующих состояниях, исполняя случайную политику в среде.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): Среда, в которой собираются данные.\n",
    "            num_episodes (int): Количество эпизодов для сбора данных.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Возвращает кортеж из трех массивов (states, actions, next_states).\n",
    "        \"\"\"\n",
    "        states, actions, next_states = [], [], []\n",
    "        for _ in tqdm(range(num_episodes)):\n",
    "            state, info = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                next_states.append(next_state)\n",
    "                state = next_state\n",
    "        return np.array(states), np.array(actions), np.array(next_states)\n",
    "\n",
    "    def choose_action(self, state, rollout, horizon):\n",
    "        initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False)\n",
    "        best_action = None\n",
    "        lowest_cost = float('inf')\n",
    "        \n",
    "        # Initialize action as a tensor with requires_grad=True to enable gradient computation\n",
    "        action = torch.zeros(1, 1, requires_grad=True)\n",
    "        \n",
    "        # Use an optimizer; here we use Adam for its robustness\n",
    "        optimizer = optim.Adam([action], lr=0.1)\n",
    "        \n",
    "        for trajectory in range(rollout):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            trajectory_cost = 0\n",
    "            state = initial_state\n",
    "            \n",
    "            for h in range(horizon):\n",
    "                if h == 0:\n",
    "                    first_action = action.clone()\n",
    "                \n",
    "                # Forward pass through the system model\n",
    "                next_state = self.system_model(torch.cat([state, action], dim=-1))\n",
    "                \n",
    "                # Compute cost\n",
    "                cost = self.cost_function(next_state, action)\n",
    "                trajectory_cost += cost\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            # Backward pass to compute gradient of the cost wrt action\n",
    "            trajectory_cost.backward()\n",
    "            \n",
    "            # Update action using its gradient\n",
    "            optimizer.step()\n",
    "            \n",
    "            if trajectory_cost < lowest_cost:\n",
    "                lowest_cost = trajectory_cost\n",
    "                best_action = first_action.detach()\n",
    "        \n",
    "        return best_action.numpy()\n",
    "\n",
    "\n",
    "    def choose_action_ref(self, state, rollout, horizon, reference_signals, step):\n",
    "        \"\"\"\n",
    "        Выбирает оптимальное действие с учетом эталонных сигналов.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): Текущее состояние среды.\n",
    "            rollout (int): Количество прогнозируемых траекторий для оценки.\n",
    "            horizon (int): Горизонт планирования.\n",
    "            reference_signals (numpy.ndarray): Эталонные сигналы для оценки действий.\n",
    "            step (int): Текущий временной шаг в среде.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Возвращает массив, содержащий выбранное действие.\n",
    "        \"\"\"\n",
    "        initial_state = torch.tensor([state], dtype=torch.float32)\n",
    "        best_action = None\n",
    "        max_trajectory_value = -float('inf')\n",
    "        action_distribution = Uniform(-60, 60)\n",
    "        for trajectory in range(rollout):\n",
    "            state = initial_state\n",
    "            trajectory_value = 0\n",
    "            for h in range(horizon):\n",
    "                \n",
    "                action = torch.Tensor([[action_distribution.sample()]])\n",
    "                if h == 0:\n",
    "                    first_action = action\n",
    "                next_state = self.system_model(torch.cat([state, action], dim=-1))\n",
    "                costs = self.cost_function(next_state, action, reference_signals, step)\n",
    "                trajectory_value += -costs\n",
    "                \n",
    "                state = next_state\n",
    "            if trajectory_value > max_trajectory_value:\n",
    "                max_trajectory_value = trajectory_value\n",
    "                best_action = first_action\n",
    "        return best_action.numpy()\n",
    "    \n",
    "    def test_model(self, env, num_episodes=100, rollout=10, horizon=1):\n",
    "        \"\"\"\n",
    "        Тестирует модель в среде, измеряя среднее вознаграждение за серию эпизодов.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): Среда для тестирования.\n",
    "            num_episodes (int): Количество эпизодов для тестирования.\n",
    "            rollout (int): Количество прогнозируемых траекторий для выбора действий.\n",
    "            horizon (int): Горизонт планирования для выбора действий.\n",
    "\n",
    "        Returns:\n",
    "            list: Список суммарных вознаграждений за каждый эпизод.\n",
    "        \"\"\"\n",
    "        total_rewards = []  # Список для хранения суммарных вознаграждений за каждый эпизод\n",
    "        for episode in range(num_episodes):\n",
    "            state, info = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state, rollout, horizon)\n",
    "                state, reward, terminated, truncated, info= env.step(action[0])\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            print(f'Episode {episode+1}: Total Reward = {total_reward}')\n",
    "            total_rewards.append(total_reward)\n",
    "\n",
    "        average_reward = sum(total_rewards) / num_episodes\n",
    "        self.writer.add_scalar('Test/AverageReward', average_reward, num_episodes)\n",
    "        return total_rewards\n",
    "    \n",
    "    def test_network(self, states, actions, next_states):\n",
    "        \"\"\"\n",
    "        Тестирует точность предсказаний модели на заданном наборе данных.\n",
    "\n",
    "        Args:\n",
    "            states (numpy.ndarray): Массив текущих состояний.\n",
    "            actions (numpy.ndarray): Массив действий.\n",
    "            next_states (numpy.ndarray): Массив следующих состояний.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.system_model.eval()  # Перевести модель в режим оценки\n",
    "        with torch.no_grad():  # Отключить вычисление градиентов\n",
    "            # Подготовка данных\n",
    "            inputs = np.hstack((states, actions.reshape(-1, 1)))\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "            true_next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            \n",
    "            # Получение предсказаний от модели\n",
    "            predicted_next_states = self.system_model(inputs)\n",
    "            \n",
    "            # Вычисление потерь (среднеквадратичная ошибка)\n",
    "            mse_loss = torch.nn.functional.mse_loss(predicted_next_states, true_next_states)\n",
    "            print(f'Test MSE Loss: {mse_loss.item()}')\n",
    "            \n",
    "            # Логирование потерь в TensorBoard\n",
    "            self.writer.add_scalar('Test/MSE_Loss', mse_loss.item(), 0)\n",
    "        \n",
    "        self.system_model.train()  # Вернуть модель в режим обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.38845602,  0.69884235], dtype=float32), {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = model.to(device=torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_cost_function(state, action):\n",
    "    theta = state[0, 0].item()\n",
    "    theta_dot = state[0, 1].item()\n",
    "    return (theta ** 2 + 0.1 * theta_dot ** 2 + 0.001 * (action ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MPCAgent(gamma=0.99, action_dim=1, observation_dim=2, model=model, cost_function=example_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 180.10it/s]\n"
     ]
    }
   ],
   "source": [
    "states, actions, next_states = agent.collect_data(env, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.039568062871694565:  68%|██████▊   | 68/100 [01:32<00:43,  1.37s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 103\u001b[0m, in \u001b[0;36mMPCAgent.train_model\u001b[0;34m(self, states, actions, next_states, epochs, batch_size)\u001b[0m\n\u001b[1;32m    101\u001b[0m inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((batch_states, batch_actions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    102\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_next_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_model_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    105\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_model(inputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train_model(states, actions, next_states, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:15<00:00, 194.05it/s]\n"
     ]
    }
   ],
   "source": [
    "states, actions, next_states = agent.collect_data(env, num_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 0.00246351957321167\n"
     ]
    }
   ],
   "source": [
    "agent.test_network(states, actions, next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, rollout, horizon):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32)\n",
    "    best_action = None\n",
    "    max_trajectory_value = -float('inf')\n",
    "    action_distribution =torch.distributions.uniform.Uniform(-2,2)\n",
    "\n",
    "\n",
    "    for trajectory in range(rollout):\n",
    "        state = initial_state\n",
    "        trajectory_value = 0\n",
    "        for h in range(horizon):\n",
    "            action = torch.Tensor([[action_distribution.sample()]])\n",
    "            if h == 0:\n",
    "                first_action = action\n",
    "            next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "            costs = agent.cost_function(next_state, action)\n",
    "            trajectory_value += -costs\n",
    "                \n",
    "            state = next_state\n",
    "        if trajectory_value > max_trajectory_value:\n",
    "            max_trajectory_value = trajectory_value\n",
    "            best_action = first_action\n",
    "    return best_action.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_gradient(state, rollout, horizon, lr=1):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False)\n",
    "    action = torch.zeros(1, 1, requires_grad=True)  # Start with a zero action\n",
    "    optimizer = torch.optim.Adam([action], lr=lr)  # Use Adam optimizer for the action variable\n",
    "\n",
    "    for step in range(rollout):\n",
    "        optimizer.zero_grad()\n",
    "        state = initial_state\n",
    "        cumulative_cost = 0\n",
    "        for h in range(horizon):\n",
    "            next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "            cost = agent.cost_function(next_state, action)\n",
    "            cumulative_cost += cost\n",
    "            state = next_state\n",
    "\n",
    "        # Since we want to minimize the cost, we take negative of the cumulative cost\n",
    "        (-cumulative_cost).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Clip or adjust the action as per the action space constraints\n",
    "    optimized_action = action.detach().numpy()\n",
    "    return optimized_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_grad_opti(state, rollout, horizon):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False)\n",
    "    best_cost = float('inf')\n",
    "    best_action_sequence = None\n",
    "\n",
    "    for _ in range(rollout):\n",
    "        action_sequence = torch.randn(horizon, 1, requires_grad=True)  # Инициализируем последовательность действий\n",
    "        optimizer = optim.Adam([action_sequence], lr=1)\n",
    "\n",
    "        for optimization_step in range(rollout):  # Количество шагов оптимизации\n",
    "            optimizer.zero_grad()\n",
    "            state = initial_state\n",
    "            total_cost = 0\n",
    "            for h in range(horizon):\n",
    "                action = action_sequence[h].unsqueeze(0)\n",
    "                next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "                cost = agent.cost_function(next_state, action)\n",
    "                total_cost += cost\n",
    "                state = next_state\n",
    "\n",
    "            if total_cost < best_cost:\n",
    "                best_cost = total_cost\n",
    "                best_action_sequence = action_sequence.detach().clone()\n",
    "\n",
    "            total_cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return best_action_sequence[0].detach().numpy()  # Возвращаем первое действие из наилучшей последовательности\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_with_noise(state, rollout, horizon):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False, device=torch.device(\"mps\"))\n",
    "    best_cost = float('inf')\n",
    "    best_action_sequence = None\n",
    "\n",
    "    for _ in range(rollout):\n",
    "        # Инициализация последовательности действий с возможностью градиентного спуска\n",
    "        action_sequence = torch.randn(horizon, 1, requires_grad=True, device=torch.device(\"mps\"))\n",
    "        optimizer = optim.Adam([action_sequence], lr=0.1)\n",
    "\n",
    "        for optimization_step in range(100):  # Количество шагов оптимизации\n",
    "            optimizer.zero_grad()\n",
    "            state = initial_state\n",
    "            total_cost = 0\n",
    "            for h in range(horizon):\n",
    "                action = action_sequence[h].unsqueeze(0)\n",
    "                \n",
    "                # Добавление возмущения к действию (исправлено, чтобы избежать операции на месте)\n",
    "                noise = torch.randn_like(action) * 0.1  # Малое стандартное отклонение для возмущения\n",
    "                action = action + noise  # Исправлено на операцию, не изменяющую переменную на месте\n",
    "\n",
    "                next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "                cost = agent.cost_function(next_state, action)\n",
    "                total_cost += cost\n",
    "                state = next_state\n",
    "\n",
    "            if total_cost < best_cost:\n",
    "                best_cost = total_cost.item()\n",
    "                best_action_sequence = action_sequence.detach().clone()\n",
    "\n",
    "            total_cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Возвращаем первое действие из наилучшей найденной последовательности\n",
    "    return best_action_sequence[0].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/asmazaev/Projects/TensorAeroSpace/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:178: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n",
      "/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m----> 9\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_action_with_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[1;32m     11\u001b[0m     state, reward, terminated, truncated, info\u001b[38;5;241m=\u001b[39m env_rec\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[75], line 22\u001b[0m, in \u001b[0;36mchoose_action_with_noise\u001b[0;34m(state, rollout, horizon)\u001b[0m\n\u001b[1;32m     19\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(action) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Малое стандартное отклонение для возмущения\u001b[39;00m\n\u001b[1;32m     20\u001b[0m action \u001b[38;5;241m=\u001b[39m action \u001b[38;5;241m+\u001b[39m noise  \u001b[38;5;66;03m# Исправлено на операцию, не изменяющую переменную на месте\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m cost \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mcost_function(next_state, action)\n\u001b[1;32m     24\u001b[0m total_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\n",
      "File \u001b[0;32m~/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Выполняет прямое распространение входных данных через сеть.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Предсказание следующего состояния системы.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81, render_mode=\"rgb_array\")\n",
    "env_rec =gym.wrappers.RecordVideo(env, \"./video\")\n",
    "rollout, horizon = 50,10\n",
    "state, info = env_rec.reset()\n",
    "done = False\n",
    "episode_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = choose_action_with_noise(state, rollout, horizon)\n",
    "    print(action)\n",
    "    state, reward, terminated, truncated, info= env_rec.step(action)\n",
    "    done = terminated or truncated\n",
    "    env_rec.render()\n",
    "    episode_reward += reward\n",
    "print(episode_reward)\n",
    "env_rec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is MPS even available? macOS 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# Was the current version of PyTorch built with MPS activated?\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout: 200, horizon: 10, episode: 0, reward: -967\n"
     ]
    }
   ],
   "source": [
    "rollout, horizon = 200,10\n",
    "for episode in range(1):\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(state, rollout, horizon)\n",
    "        state, reward, terminated, truncated, info= env.step(action[0])\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    print('rollout: %d, horizon: %d, episode: %d, reward: %d' % (rollout, horizon, episode, episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10, 0, -967.0516866740916)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout, horizon, episode, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
