{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/asmazaev/Projects/TensorAeroSpace\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Uniform\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Создает нейронную сеть для моделирования динамики системы.\n",
    "\n",
    "    Сеть состоит из трех линейных слоев и функций активации ReLU между ними.\n",
    "    Входной слой принимает вектор из 3 элементов, представляющих состояния системы.\n",
    "    Второй и третий слои - это скрытые слои с 128 нейронами.\n",
    "    Выходной слой генерирует вектор из 2 элементов, представляющих предсказание следующего состояния системы.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 256)  # 3 состояния + 1 действие = 4\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)  # Предсказание следующего состояния\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Выполняет прямое распространение входных данных через сеть.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Входные данные, представляющие состояния системы.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Предсказание следующего состояния системы.\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MPCAgent(object):\n",
    "    \"\"\"\n",
    "    Агент, использующий метод Модельно-Прогностического Управления (MPC) для оптимизации действий в среде.\n",
    "\n",
    "    Attributes:\n",
    "        gamma (float): Коэффициент дисконтирования.\n",
    "        action_dim (int): Размерность пространства действий.\n",
    "        observation_dim (int): Размерность пространства наблюдений.\n",
    "        model (torch.nn.Module): Модель для аппроксимации динамики среды.\n",
    "        cost_function (callable): Функция стоимости, используемая для оценки действий.\n",
    "        lr (float): Скорость обучения для оптимизатора модели.\n",
    "        criterion (torch.nn.modules.loss): Критерий потерь для обучения модели.\n",
    "\n",
    "    Methods:\n",
    "        train_model(states, actions, next_states, epochs=100, batch_size=64):\n",
    "            Обучает модель динамики среды.\n",
    "        collect_data(env, num_episodes=1000):\n",
    "            Собирает данные о состояниях, действиях и следующих состояниях, исполняя политику в среде.\n",
    "        choose_action(state, rollout, horizon):\n",
    "            Выбирает оптимальное действие, используя прогнозируемое моделирование.\n",
    "        choose_action_ref(state, rollout, horizon, reference_signals, step):\n",
    "            Выбирает оптимальное действие с учетом эталонных сигналов.\n",
    "        test_model(env, num_episodes=100, rollout=10, horizon=1):\n",
    "            Тестирует модель, измеряя среднее вознаграждение в среде.\n",
    "        test_network(states, actions, next_states):\n",
    "            Тестирует точность предсказаний модели на заданном наборе данных.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, action_dim, observation_dim, model, cost_function, lr=1e-3, criterion=torch.nn.MSELoss()):\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "        self.observation_dim = observation_dim\n",
    "        self.system_model = model\n",
    "        self.system_model_optimizer = optim.Adam(self.system_model.parameters(), lr=lr)\n",
    "        self.cost_function = cost_function\n",
    "        self.writer = SummaryWriter()\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def train_model(self, states, actions, next_states, epochs=100, batch_size=64):\n",
    "        \"\"\"\n",
    "        Обучает модель динамики среды, используя данные о состояниях, действиях и следующих состояниях.\n",
    "\n",
    "        Args:\n",
    "            states (numpy.ndarray): Массив текущих состояний.\n",
    "            actions (numpy.ndarray): Массив действий, совершенных в этих состояниях.\n",
    "            next_states (numpy.ndarray): Массив следующих состояний после совершения действий.\n",
    "            epochs (int): Количество эпох обучения.\n",
    "            batch_size (int): Размер батча для обучения.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for epoch in  (pbar := tqdm(range(epochs))):\n",
    "            permutation = np.random.permutation(states.shape[0])\n",
    "            for i in range(0, states.shape[0], batch_size):\n",
    "                indices = permutation[i:i+batch_size]\n",
    "                batch_states, batch_actions, batch_next_states = states[indices], actions[indices], next_states[indices]\n",
    "                inputs = np.hstack((batch_states, batch_actions.reshape(-1, 1)))\n",
    "                inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "                targets = torch.tensor(batch_next_states, dtype=torch.float32)\n",
    "                self.system_model_optimizer.zero_grad()\n",
    "                outputs = self.system_model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                self.system_model_optimizer.step()\n",
    "            \n",
    "            self.writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "            pbar.set_description(f\"Loss {loss.item()}\")\n",
    "\n",
    "    def collect_data(self, env, num_episodes=1000):\n",
    "        \"\"\"\n",
    "        Собирает данные о состояниях, действиях и следующих состояниях, исполняя случайную политику в среде.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): Среда, в которой собираются данные.\n",
    "            num_episodes (int): Количество эпизодов для сбора данных.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Возвращает кортеж из трех массивов (states, actions, next_states).\n",
    "        \"\"\"\n",
    "        states, actions, next_states = [], [], []\n",
    "        for _ in tqdm(range(num_episodes)):\n",
    "            state, info = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                next_states.append(next_state)\n",
    "                state = next_state\n",
    "        return np.array(states), np.array(actions), np.array(next_states)\n",
    "\n",
    "    def choose_action(self, state, rollout, horizon):\n",
    "        initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False)\n",
    "        best_action = None\n",
    "        lowest_cost = float('inf')\n",
    "        \n",
    "        # Initialize action as a tensor with requires_grad=True to enable gradient computation\n",
    "        action = torch.zeros(1, 1, requires_grad=True)\n",
    "        \n",
    "        # Use an optimizer; here we use Adam for its robustness\n",
    "        optimizer = optim.Adam([action], lr=0.1)\n",
    "        \n",
    "        for trajectory in range(rollout):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            trajectory_cost = 0\n",
    "            state = initial_state\n",
    "            \n",
    "            for h in range(horizon):\n",
    "                if h == 0:\n",
    "                    first_action = action.clone()\n",
    "                \n",
    "                # Forward pass through the system model\n",
    "                next_state = self.system_model(torch.cat([state, action], dim=-1))\n",
    "                \n",
    "                # Compute cost\n",
    "                cost = self.cost_function(next_state, action)\n",
    "                trajectory_cost += cost\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            # Backward pass to compute gradient of the cost wrt action\n",
    "            trajectory_cost.backward()\n",
    "            \n",
    "            # Update action using its gradient\n",
    "            optimizer.step()\n",
    "            \n",
    "            if trajectory_cost < lowest_cost:\n",
    "                lowest_cost = trajectory_cost\n",
    "                best_action = first_action.detach()\n",
    "        \n",
    "        return best_action.numpy()\n",
    "\n",
    "\n",
    "    def choose_action_ref(self, state, rollout, horizon, reference_signals, step):\n",
    "        \"\"\"\n",
    "        Выбирает оптимальное действие с учетом эталонных сигналов.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): Текущее состояние среды.\n",
    "            rollout (int): Количество прогнозируемых траекторий для оценки.\n",
    "            horizon (int): Горизонт планирования.\n",
    "            reference_signals (numpy.ndarray): Эталонные сигналы для оценки действий.\n",
    "            step (int): Текущий временной шаг в среде.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Возвращает массив, содержащий выбранное действие.\n",
    "        \"\"\"\n",
    "        initial_state = torch.tensor([state], dtype=torch.float32)\n",
    "        best_action = None\n",
    "        max_trajectory_value = -float('inf')\n",
    "        action_distribution = Uniform(-60, 60)\n",
    "        for trajectory in range(rollout):\n",
    "            state = initial_state\n",
    "            trajectory_value = 0\n",
    "            for h in range(horizon):\n",
    "                \n",
    "                action = torch.Tensor([[action_distribution.sample()]])\n",
    "                if h == 0:\n",
    "                    first_action = action\n",
    "                next_state = self.system_model(torch.cat([state, action], dim=-1))\n",
    "                costs = self.cost_function(next_state, action, reference_signals, step)\n",
    "                trajectory_value += -costs\n",
    "                \n",
    "                state = next_state\n",
    "            if trajectory_value > max_trajectory_value:\n",
    "                max_trajectory_value = trajectory_value\n",
    "                best_action = first_action\n",
    "        return best_action.numpy()\n",
    "    \n",
    "    def test_model(self, env, num_episodes=100, rollout=10, horizon=1):\n",
    "        \"\"\"\n",
    "        Тестирует модель в среде, измеряя среднее вознаграждение за серию эпизодов.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): Среда для тестирования.\n",
    "            num_episodes (int): Количество эпизодов для тестирования.\n",
    "            rollout (int): Количество прогнозируемых траекторий для выбора действий.\n",
    "            horizon (int): Горизонт планирования для выбора действий.\n",
    "\n",
    "        Returns:\n",
    "            list: Список суммарных вознаграждений за каждый эпизод.\n",
    "        \"\"\"\n",
    "        total_rewards = []  # Список для хранения суммарных вознаграждений за каждый эпизод\n",
    "        for episode in range(num_episodes):\n",
    "            state, info = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state, rollout, horizon)\n",
    "                state, reward, terminated, truncated, info= env.step(action[0])\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            print(f'Episode {episode+1}: Total Reward = {total_reward}')\n",
    "            total_rewards.append(total_reward)\n",
    "\n",
    "        average_reward = sum(total_rewards) / num_episodes\n",
    "        self.writer.add_scalar('Test/AverageReward', average_reward, num_episodes)\n",
    "        return total_rewards\n",
    "    \n",
    "    def test_network(self, states, actions, next_states):\n",
    "        \"\"\"\n",
    "        Тестирует точность предсказаний модели на заданном наборе данных.\n",
    "\n",
    "        Args:\n",
    "            states (numpy.ndarray): Массив текущих состояний.\n",
    "            actions (numpy.ndarray): Массив действий.\n",
    "            next_states (numpy.ndarray): Массив следующих состояний.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.system_model.eval()  # Перевести модель в режим оценки\n",
    "        with torch.no_grad():  # Отключить вычисление градиентов\n",
    "            # Подготовка данных\n",
    "            inputs = np.hstack((states, actions.reshape(-1, 1)))\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "            true_next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            \n",
    "            # Получение предсказаний от модели\n",
    "            predicted_next_states = self.system_model(inputs)\n",
    "            \n",
    "            # Вычисление потерь (среднеквадратичная ошибка)\n",
    "            mse_loss = torch.nn.functional.mse_loss(predicted_next_states, true_next_states)\n",
    "            print(f'Test MSE Loss: {mse_loss.item()}')\n",
    "            \n",
    "            # Логирование потерь в TensorBoard\n",
    "            self.writer.add_scalar('Test/MSE_Loss', mse_loss.item(), 0)\n",
    "        \n",
    "        self.system_model.train()  # Вернуть модель в режим обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.02761483, -0.36976734], dtype=float32), {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_cost_function(state, action):\n",
    "    theta = state[0, 0].item()\n",
    "    theta_dot = state[0, 1].item()\n",
    "    return (theta ** 2 + 0.1 * theta_dot ** 2 + 0.001 * (action ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MPCAgent(gamma=0.99, action_dim=1, observation_dim=2, model=model, cost_function=example_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:09<00:00, 200.13it/s]\n"
     ]
    }
   ],
   "source": [
    "states, actions, next_states = agent.collect_data(env, num_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0019631721079349518: 100%|██████████| 300/300 [03:35<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "agent.train_model(states, actions, next_states, epochs=300, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:15<00:00, 199.96it/s]\n"
     ]
    }
   ],
   "source": [
    "states, actions, next_states = agent.collect_data(env, num_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 0.005056232679635286\n"
     ]
    }
   ],
   "source": [
    "agent.test_network(states, actions, next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, rollout, horizon):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32)\n",
    "    best_action = None\n",
    "    max_trajectory_value = -float('inf')\n",
    "    action_distribution =torch.distributions.uniform.Uniform(-2,2)\n",
    "\n",
    "\n",
    "    for trajectory in range(rollout):\n",
    "        state = initial_state\n",
    "        trajectory_value = 0\n",
    "        for h in range(horizon):\n",
    "            action = torch.Tensor([[action_distribution.sample()]])\n",
    "            if h == 0:\n",
    "                first_action = action\n",
    "            next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "            costs = agent.cost_function(next_state, action)\n",
    "            trajectory_value += -costs\n",
    "                \n",
    "            state = next_state\n",
    "        if trajectory_value > max_trajectory_value:\n",
    "            max_trajectory_value = trajectory_value\n",
    "            best_action = first_action\n",
    "    return best_action.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_gradient(state, rollout, horizon, lr=1):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False)\n",
    "    action = torch.zeros(1, 1, requires_grad=True)  # Start with a zero action\n",
    "    optimizer = torch.optim.Adam([action], lr=lr)  # Use Adam optimizer for the action variable\n",
    "\n",
    "    for step in range(rollout):\n",
    "        optimizer.zero_grad()\n",
    "        state = initial_state\n",
    "        cumulative_cost = 0\n",
    "        for h in range(horizon):\n",
    "            next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "            cost = agent.cost_function(next_state, action)\n",
    "            cumulative_cost += cost\n",
    "            state = next_state\n",
    "\n",
    "        # Since we want to minimize the cost, we take negative of the cumulative cost\n",
    "        (-cumulative_cost).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Clip or adjust the action as per the action space constraints\n",
    "    optimized_action = action.detach().numpy()\n",
    "    return optimized_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_grad_opti(state, rollout, horizon):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False)\n",
    "    best_cost = float('inf')\n",
    "    best_action_sequence = None\n",
    "\n",
    "    for _ in range(rollout):\n",
    "        action_sequence = torch.randn(horizon, 1, requires_grad=True)  # Инициализируем последовательность действий\n",
    "        optimizer = optim.Adam([action_sequence], lr=1)\n",
    "\n",
    "        for optimization_step in range(rollout):  # Количество шагов оптимизации\n",
    "            optimizer.zero_grad()\n",
    "            state = initial_state\n",
    "            total_cost = 0\n",
    "            for h in range(horizon):\n",
    "                action = action_sequence[h].unsqueeze(0)\n",
    "                next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "                cost = agent.cost_function(next_state, action)\n",
    "                total_cost += cost\n",
    "                state = next_state\n",
    "\n",
    "            if total_cost < best_cost:\n",
    "                best_cost = total_cost\n",
    "                best_action_sequence = action_sequence.detach().clone()\n",
    "\n",
    "            total_cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return best_action_sequence[0].detach().numpy()  # Возвращаем первое действие из наилучшей последовательности\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_with_noise(state, rollout, horizon):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False)\n",
    "    best_cost = float('inf')\n",
    "    best_action_sequence = None\n",
    "\n",
    "    for _ in range(rollout):\n",
    "        # Инициализация последовательности действий с возможностью градиентного спуска\n",
    "        action_sequence = torch.randn(horizon, 1, requires_grad=True)\n",
    "        optimizer = optim.Adam([action_sequence], lr=0.1)\n",
    "\n",
    "        for optimization_step in range(100):  # Количество шагов оптимизации\n",
    "            optimizer.zero_grad()\n",
    "            state = initial_state\n",
    "            total_cost = 0\n",
    "            for h in range(horizon):\n",
    "                action = action_sequence[h].unsqueeze(0)\n",
    "                \n",
    "                # Добавление возмущения к действию (исправлено, чтобы избежать операции на месте)\n",
    "                noise = torch.randn_like(action) * 0.1  # Малое стандартное отклонение для возмущения\n",
    "                action = action + noise  # Исправлено на операцию, не изменяющую переменную на месте\n",
    "\n",
    "                next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "                cost = agent.cost_function(next_state, action)\n",
    "                total_cost += cost\n",
    "                state = next_state\n",
    "\n",
    "            if total_cost < best_cost:\n",
    "                best_cost = total_cost.item()\n",
    "                best_action_sequence = action_sequence.detach().clone()\n",
    "\n",
    "            total_cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Возвращаем первое действие из наилучшей найденной последовательности\n",
    "    return best_action_sequence[0].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_action_with_noise_v1(state, rollout, horizon):\n",
    "    initial_state = torch.tensor([state], dtype=torch.float32, requires_grad=False)\n",
    "    best_cost = float('inf')\n",
    "    best_action_sequence = None\n",
    "\n",
    "    # Предварительное вычисление шума для всех возможных действий\n",
    "    noise = torch.randn(rollout, horizon, 1) * 0.1\n",
    "\n",
    "    for rollout_index in range(rollout):\n",
    "        # Инициализация последовательности действий с возможностью градиентного спуска\n",
    "        action_sequence = torch.randn(horizon, 1, requires_grad=True)\n",
    "        optimizer = optim.Adam([action_sequence], lr=0.1)\n",
    "\n",
    "        for optimization_step in range(100):  # Количество шагов оптимизации\n",
    "            optimizer.zero_grad()\n",
    "            state = initial_state.clone()\n",
    "            total_cost = 0\n",
    "\n",
    "            # Добавление шума заранее\n",
    "            action_sequence_with_noise = action_sequence + noise[rollout_index]\n",
    "\n",
    "            for h in range(horizon):\n",
    "                action = action_sequence_with_noise[h].unsqueeze(0)\n",
    "                noise = torch.randn_like(action) * 0.1\n",
    "                next_state = agent.system_model(torch.cat([state, action], dim=-1))\n",
    "                cost = agent.cost_function(next_state, action)\n",
    "                total_cost += cost\n",
    "                state = next_state.detach()  # Отсоединяем, чтобы избежать ненужного накопления градиентов\n",
    "\n",
    "            if total_cost < best_cost:\n",
    "                best_cost = total_cost.item()\n",
    "                best_action_sequence = action_sequence.detach().clone()\n",
    "\n",
    "            total_cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return best_action_sequence[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActionGenerator(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, horizon):\n",
    "        super(ActionGenerator, self).__init__()\n",
    "        self.horizon = horizon\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim * horizon)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.tanh(self.fc1(state))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        action_sequence = torch.tanh(self.fc3(x))\n",
    "        action_sequence = action_sequence.view(-1, self.horizon, 1)  # Reshape to [horizon, action_dim]\n",
    "        return action_sequence\n",
    "\n",
    "action_generator = ActionGenerator(2, 1, 10)\n",
    "optimizer_action = optim.Adam(action_generator.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_with_network(state, horizon, rollout=10):\n",
    "    best_cost = float('inf')\n",
    "    best_action_sequence = None\n",
    "\n",
    "    for rollout_iter in range(rollout):\n",
    "        optimizer_action.zero_grad()  # Очищаем градиенты на каждом роллауте\n",
    "\n",
    "        action_sequence = action_generator(torch.tensor([state], dtype=torch.float32)) \n",
    "        total_cost = torch.tensor(0., requires_grad=True)  # Инициализируем total_cost как тензор для накопления градиентов\n",
    "        state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "\n",
    "        for h in range(horizon):\n",
    "            action = action_sequence[:, h, :]\n",
    "            noise = torch.randn_like(action) * 0.1\n",
    "            action = (action + noise)\n",
    "            actions_clipped = torch.clip(action, -2, 2)\n",
    "            next_state = agent.system_model(torch.cat([state_tensor, actions_clipped], dim=-1))\n",
    "            cost = agent.cost_function(next_state, actions_clipped)\n",
    "\n",
    "            if cost.requires_grad:\n",
    "                cost.backward(retain_graph=True)  # Накапливаем градиенты на каждом шаге\n",
    "\n",
    "            total_cost = total_cost + cost.detach().item()  # Обновляем total_cost, используя значение стоимости\n",
    "            state_tensor = next_state.detach()\n",
    "\n",
    "        # Проверяем, является ли найденная последовательность действий лучшей\n",
    "        if total_cost < best_cost:\n",
    "            best_cost = total_cost\n",
    "            best_action_sequence = action_sequence.detach().clone()\n",
    "\n",
    "    # После всех роллаутов, делаем шаг оптимизации\n",
    "    optimizer_action.step()\n",
    "\n",
    "    return best_action_sequence[0, 0].numpy()  # Возвращаем первое действие из лучшей последовательности\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/asmazaev/Projects/TensorAeroSpace/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "100%|█████████▉| 199/200 [00:22<00:00,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:23<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-0.mp4\n",
      "-1089.3551462689184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 199/200 [00:22<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-1.mp4\n",
      "-1817.5463590224992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-862.1822102561072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1689.1850605552236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-861.7928944045152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1190.9053250696911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-848.5041158585544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1273.4843445141641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 199/200 [00:23<00:00,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:23<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-8.mp4\n",
      "-748.7775248185852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1626.43625899645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-821.8756828588789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1185.503173078708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1169.919511180048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1453.5951487384955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1540.404016595317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1789.6466608823462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1007.2592278410323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-865.8452123515995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1755.6243432157637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1162.4545046941728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1267.095094378468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1398.5068305315845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-953.6589352224665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1619.3457175416804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1001.5421486693716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-629.1082381225402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-953.7574418620668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 199/200 [00:22<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-27.mp4.\n",
      "Moviepy - Writing video /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-27.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/asmazaev/Projects/TensorAeroSpace/video/rl-video-episode-27.mp4\n",
      "-1473.7103607311676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-974.1388025820218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1720.6918680221522\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81, render_mode=\"rgb_array\")\n",
    "env_rec =gym.wrappers.RecordVideo(env, \"./video\")\n",
    "rollout, horizon = 50,10\n",
    "\n",
    "\n",
    "for _i in range(30):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    state, info = env_rec.reset()\n",
    "    for _ in tqdm(range(200)):\n",
    "        action = choose_action_with_network(state, horizon, rollout)\n",
    "        state, reward, terminated, truncated, info= env_rec.step(action)\n",
    "        done = terminated or truncated\n",
    "        env_rec.render()\n",
    "        episode_reward += reward\n",
    "    print(episode_reward)\n",
    "env_rec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is MPS even available? macOS 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# Was the current version of PyTorch built with MPS activated?\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout: 200, horizon: 10, episode: 0, reward: -967\n"
     ]
    }
   ],
   "source": [
    "rollout, horizon = 200,10\n",
    "for episode in range(1):\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(state, rollout, horizon)\n",
    "        state, reward, terminated, truncated, info= env.step(action[0])\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    print('rollout: %d, horizon: %d, episode: %d, reward: %d' % (rollout, horizon, episode, episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10, 0, -967.0516866740916)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout, horizon, episode, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
