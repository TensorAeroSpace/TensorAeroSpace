{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, minibatch_size, observation_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        #random.seed(seed)\n",
    "        self.max_size = buffer_size\n",
    "        self.pos = 0\n",
    "        self.full = False\n",
    "        self.states = np.zeros((self.max_size,observation_size))\n",
    "        self.next_states = np.zeros((self.max_size,observation_size))\n",
    "        self.actions = np.zeros(self.max_size,dtype=np.int8)\n",
    "        self.rewards = np.zeros(self.max_size)\n",
    "        self.terminals = np.zeros(self.max_size,dtype=np.int8)\n",
    "\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        self.states[self.pos] = state\n",
    "        self.actions[self.pos] = action\n",
    "        self.rewards[self.pos] = reward\n",
    "        self.terminals[self.pos] = terminal\n",
    "        self.next_states[self.pos] = next_state\n",
    "        self.pos += 1\n",
    "        if(self.pos==self.max_size):\n",
    "            self.pos = 0\n",
    "            self.full = True\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        if(self.full):\n",
    "            idxs = np.random.randint(0,self.max_size,size=self.minibatch_size) \n",
    "        else:\n",
    "            idxs = np.random.randint(0,self.pos,size=self.minibatch_size)\n",
    "        sample_ = [self.states[idxs],self.actions[idxs],self.rewards[idxs],self.terminals[idxs],\n",
    "                   self.next_states[idxs]]\n",
    "        #print(sample_)\n",
    "        return sample_\n",
    "\n",
    "    def size(self):\n",
    "        if(self.full):\n",
    "            return self.max_size\n",
    "        else:\n",
    "            return self.pos\n",
    "    \n",
    "    def reset(self):\n",
    "        self.full = False\n",
    "        self.pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self,network_config) -> None:\n",
    "        super(ActorCritic,self).__init__()\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(self.state_dim, self.num_hidden_units)\n",
    "        self.layer2 = torch.nn.Linear(self.num_hidden_units,self.num_hidden_units)\n",
    "        self.policy_layer = torch.nn.Linear(self.num_hidden_units, self.num_actions)\n",
    "        self.value_layer = torch.nn.Linear(self.num_hidden_units,1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self.layer1,self.relu,self.policy_layer,self.softmax\n",
    "        )\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self.layer1,self.relu,self.value_layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        policy = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return policy,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_error_double_dqn(states, next_states, actions, rewards, discount, terminals, target_network, current_q_network):\n",
    "    with torch.no_grad():\n",
    "        # The idea of Double DQN is to get max actions from current network\n",
    "        # and to get Q values from target_network for next states. \n",
    "        q_next_mat = current_q_network(next_states)\n",
    "        max_actions = torch.argmax(q_next_mat,1)\n",
    "        double_q_mat = target_network(next_states)\n",
    "    batch_indices = torch.arange(q_next_mat.shape[0])\n",
    "    double_q_max = double_q_mat[batch_indices,max_actions]\n",
    "    target_vec = rewards+discount*double_q_max*(torch.ones_like(terminals)-terminals)\n",
    "    q_mat = current_q_network(states)\n",
    "    batch_indices = torch.arange(q_mat.shape[0])\n",
    "    q_vec = q_mat[batch_indices,actions]\n",
    "    #delta_vec = target_vec - q_vec\n",
    "    return target_vec,q_vec\n",
    "\n",
    "def get_td_error_dqn(states, next_states, actions, rewards, discount, terminals, target_network, current_q_network):\n",
    "    with torch.no_grad():\n",
    "        q_next_mat = target_network(next_states)\n",
    "        q_max = torch.max(q_next_mat,1)[0]\n",
    "    batch_indices = torch.arange(q_next_mat.shape[0])\n",
    "    target_vec = rewards+discount*q_max*(torch.ones_like(terminals)-terminals)\n",
    "    q_mat = current_q_network(states)\n",
    "    batch_indices = torch.arange(q_mat.shape[0])\n",
    "    q_vec = q_mat[batch_indices,actions]\n",
    "    return target_vec,q_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, target_network, current_q_network,device,double_dqn=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions,\n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets,\n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states = experiences[0]\n",
    "    actions = experiences[1]\n",
    "    rewards = experiences[2]\n",
    "    terminals = experiences[3]\n",
    "    next_states = experiences[4]\n",
    "    # numpy arrays to tensors\n",
    "    states = torch.tensor(states,dtype=torch.float32,device=device)\n",
    "    \n",
    "    #print(states.shape)\n",
    "    next_states = torch.tensor(next_states,dtype=torch.float32,device=device)\n",
    "    rewards = torch.tensor(rewards,dtype=torch.float32,device=device)\n",
    "    terminals = torch.tensor(terminals,dtype=torch.int,device=device)\n",
    "    actions = torch.tensor(actions,dtype=torch.int,device=device)\n",
    "    #map(list, zip(*experiences))\n",
    "    #states = torch.concatenate(states)\n",
    "    #next_states = torch.concatenate(next_states)\n",
    "    #rewards = torch.tensor(rewards,dtype=torch.float32,device=device)\n",
    "    #terminals = torch.tensor(terminals,dtype=torch.float32,device=device)\n",
    "    batch_size = states.shape[0]\n",
    "    # Compute TD error using the get_td_error function\n",
    "    # Note that q_vec is a 1D array of shape (batch_size)\n",
    "    if(double_dqn):\n",
    "        target_vec,q_vec = get_td_error_double_dqn(states, next_states, actions, rewards, discount, terminals, target_network, current_q_network)\n",
    "    else:\n",
    "        target_vec,q_vec = get_td_error_dqn(states, next_states, actions, rewards, discount, terminals, target_network, current_q_network)\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "    loss = loss_fun(target_vec,q_vec)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(target_network.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    return loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self):\n",
    "        self.name = \"A2C\"\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        self.device = None\n",
    "        self.seed = 1 # random seed. Later can be changed by using set_seed method\n",
    "\n",
    "    def set_seed(self,seed=1):\n",
    "        self.seed = seed\n",
    "        #random.seed(self.seed)\n",
    "    \n",
    "\n",
    "    def set_device(self,device):\n",
    "        self.device = device\n",
    "    \n",
    "    def agent_init(self, agent_config):\n",
    "        if(self.device==None):\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.state_dim = agent_config[\"network_config\"].get(\"state_dim\")\n",
    "        self.num_hidden_layers = agent_config[\"network_config\"].get(\"num_hidden_units\")\n",
    "        self.num_actions = agent_config[\"network_config\"].get(\"num_actions\")\n",
    "        self.dueling = agent_config[\"network_config\"].get(\"dueling\")\n",
    "        \n",
    "        self.actor_critic_network = ActorCritic(agent_config['network_config']).to(self.device)\n",
    "        \n",
    "        self.actor_step_size = 1e-4 #agent_config['actor_step_size']\n",
    "        self.critic_step_size =  1e-3 #agent_config['critic_step_size']\n",
    "        self.avg_reward_step_size = 1e-3\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.time_step = 0\n",
    "        self.loss = []\n",
    "        self.episode_rewards = []\n",
    "        self.loss_capacity = 5_000\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_critic_network.actor.parameters(),lr=self.actor_step_size)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.actor_critic_network.critic.parameters(),lr=self.critic_step_size)\n",
    "        self.avg_reward = 0\n",
    "        self.I = 1\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.terminals = []\n",
    "\n",
    "    def select_action(self,state):\n",
    "        state = torch.tensor(np.array(state),dtype=torch.float32,device=self.device)\n",
    "        policy,_ = self.actor_critic_network(state)\n",
    "        action_probabilities = policy.squeeze().detach().numpy()\n",
    "        action = np.random.choice(len(action_probabilities), p=action_probabilities)\n",
    "        return action\n",
    "\n",
    "    def policy(self,state):\n",
    "        #state = torch.tensor(state,dtype=torch.float32)\n",
    "        \n",
    "        policy,_ = self.actor_critic_network(state)\n",
    "        m = torch.distributions.Categorical(policy)\n",
    "        action = m.sample()\n",
    "        lp = m.log_prob(action)\n",
    "        #action = torch.multinomial(policy, 1).item()\n",
    "        return action.item(),lp\n",
    "    \n",
    "    def value(self,state):\n",
    "        #state = torch.tensor(state,dtype=torch.float32)\n",
    "        _,value = self.actor_critic_network(state)\n",
    "        return value\n",
    "\n",
    "    # Work Required: No.\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.I = 1\n",
    "        self.last_state = state #torch.tensor(np.array([state]),dtype=torch.float32,device=self.device)\n",
    "        self.last_action = self.select_action(state)\n",
    "        self.time_step += 1\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        action = self.select_action(state)\n",
    "        self.states.append(self.last_state)\n",
    "        self.actions.append(self.last_action)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminals.append(False)\n",
    "        self.next_states.append(state)\n",
    "        self.last_action = action\n",
    "        self.last_state = state\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        self.states.append(self.last_state)\n",
    "        self.actions.append(self.last_action)\n",
    "        self.rewards.append(reward)\n",
    "        state = np.zeros_like(self.last_state)\n",
    "        self.terminals.append(True)\n",
    "        self.next_states.append(state)\n",
    "        loss=self.agent_update()\n",
    "        return loss\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        \n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        for i in reversed(range(0,len(rewards))):\n",
    "            \n",
    "            running_add = running_add * self.discount + rewards[i]\n",
    "            discounted_r[i] = running_add\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "    def agent_update(self):\n",
    "        #discount_r = self.discount_rewards(self.rewards)\n",
    "        #policy,values = self.actor_critic_network(self.states)\n",
    "\n",
    "        #advantages = discount_r - values\n",
    "        R = self.discount_rewards(self.rewards)\n",
    "        R = torch.FloatTensor(R)\n",
    "        states = torch.FloatTensor(self.states)\n",
    "        actions = torch.LongTensor(self.actions)\n",
    "        rewards = torch.FloatTensor(self.rewards)\n",
    "        next_states = torch.FloatTensor(self.next_states)\n",
    "        dones = torch.FloatTensor(self.terminals)\n",
    "        ones = torch.ones_like(dones)\n",
    "        \n",
    "\n",
    "        # Compute advantages\n",
    "        _, next_values = self.actor_critic_network(next_states)\n",
    "        advantages = R - self.actor_critic_network.critic(states).detach().squeeze()\n",
    "\n",
    "        # Compute critic loss\n",
    "        loss_c = torch.nn.MSELoss()\n",
    "        critic_loss = loss_c(R,self.actor_critic_network.critic(states).squeeze())#, rewards + self.discount * next_values.detach().squeeze() * (ones-dones))\n",
    "\n",
    "        # Compute actor loss\n",
    "        policy, _ = self.actor_critic_network(states)\n",
    "        selected_probs = policy.gather(1, actions.unsqueeze(1))\n",
    "        actor_loss = -(torch.log(selected_probs) * advantages.detach()).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        #print(\"Actor loss\",actor_loss.detach().numpy())\n",
    "        #+print(\"Critic loss\",critic_loss.detach().numpy())\n",
    "        # Total loss\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "        return total_loss.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")\n",
    "\n",
    "    def get_loss(self):\n",
    "        return np.average(np.array(self.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Utilities:\n",
    "    def load_model(self,model,filename=\"model.weights\"):\n",
    "        if(model.name == \"DQN\"):\n",
    "            model.target_network.load_state_dict(torch.load(filename))\n",
    "            model.q_network.load_state_dict(torch.load(filename))\n",
    "        elif(model.name==\"ActorCritic\"):\n",
    "            model.actor_critic_network.load_state_dict(torch.load(filename))\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "\n",
    "    def save_model(self,model,filename=\"model.weights\"):\n",
    "        if(model.name == \"DQN\"):\n",
    "            torch.save(model.q_network.state_dict(),filename)\n",
    "        elif(model.name==\"ActorCritic\"):\n",
    "            torch.save(model.actor_critic_network.state_dict(),filename)\n",
    "\n",
    "\n",
    "    def test_agent(self,agent,test_env):\n",
    "        pass\n",
    "\n",
    "    def visualize_values(self,agent,env,runs=1):\n",
    "        for i in range(runs):\n",
    "            state,_=env.reset()\n",
    "            done = False\n",
    "            n_step = 0\n",
    "            values = []\n",
    "            while(done!=True):\n",
    "                action_values = agent.q_network.get_action_values(state)\n",
    "                values.append(action_values)\n",
    "                action = np.argmax(action_values)\n",
    "                state,reward,terminated,truncated,info=env.step(action)\n",
    "                done = terminated or truncated\n",
    "            values = np.array(values)\n",
    "            values = values.reshape(values.shape[0],agent.num_actions)\n",
    "            print(values.shape)\n",
    "            plt.plot(values)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "#import gymnasium as gym\n",
    "import numpy as np\n",
    "#from DQNAgent import DQNAgent\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "class RL:\n",
    "    def __init__(self) -> None:\n",
    "        self.name = \"ButaChanRL\"\n",
    "        self.mean_episode_length = 0\n",
    "        self.mean_episode_rew = 0\n",
    "        self.mean_loss= 0\n",
    "        self.step = 0\n",
    "        self.output_step = 1000\n",
    "        self.epsiode_rewards = []\n",
    "        self.episode_lens = []\n",
    "        self.loss = []\n",
    "        self.utils = Utilities()\n",
    "        self.model_dir = \"./models/\"\n",
    "        self.num_episodes = 0\n",
    "        self.average_over = 20\n",
    "\n",
    "    def set_output_step(self,output_step):\n",
    "        self.output_step = output_step\n",
    "\n",
    "    def set_seed(self,seed=1):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def set_model_dir(self,name):\n",
    "        self.model_dir = name\n",
    "\n",
    "    def create_model_dir(self):\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "\n",
    "    def visualize_values(self,agent,env,runs=1):\n",
    "        self.utils.visualize_values(agent,env,runs)\n",
    "     \n",
    "    def plot_live(self,data,n_mean=20,plot_start=20):\n",
    "        plt.ion()\n",
    "        plt.figure(1)\n",
    "        plot_data = torch.tensor(data, dtype=torch.float)\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode Reward')\n",
    "        plt.plot(plot_data.numpy(),\"o\")\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(plot_data ) >= plot_start:\n",
    "            means = plot_data .unfold(0, n_mean, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(n_mean), means))\n",
    "            plt.plot(means.numpy())\n",
    "        plt.pause(0.1)  # pause a bit so that plots are updated\n",
    "\n",
    "    def plot_validate(self,data,test_data):\n",
    "        plt.ion()\n",
    "        plt.figure(1)\n",
    "        plot_data = torch.tensor(data, dtype=torch.float)\n",
    "        test_data = torch.tensor(test_data,dtype=torch.float32)\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode Reward')\n",
    "        plt.plot(plot_data.numpy(),\"o\")\n",
    "        plt.plot(test_data.numpy(),\"rx\")\n",
    "        plt.pause(0.1)  # pause a bit so that plots are updated\n",
    "\n",
    "    def episode_summarize(self,episode,episode_reward):\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    def summarize(self):\n",
    "        self.mean_episode_length = 0\n",
    "        self.mean_episode_rew = 0\n",
    "        if(len(self.episode_lens)>0):\n",
    "            if(len(self.episode_lens)>self.average_over):\n",
    "                self.mean_episode_length = np.average(self.episode_lens[-self.average_over:-1])\n",
    "                self.mean_episode_rew = np.average(self.epsiode_rewards[-self.average_over:-1])\n",
    "            else:\n",
    "                self.mean_episode_length = np.average(self.episode_lens)\n",
    "                self.mean_episode_rew = np.average(self.epsiode_rewards)\n",
    "        self.mean_loss = 0\n",
    "        if(len(self.loss)>0):\n",
    "            self.mean_loss = np.average(self.loss)\n",
    "        print(f\"Step:{self.step}, Episode:{self.num_episodes} Mean_Epi_Len: {self.mean_episode_length:5.2f},Mean_Epi_Rew {self.mean_episode_rew:5.2f}, Loss: {self.mean_loss:5.2f}\")\n",
    "\n",
    "    def learn(self,agent,env,agent_parameters,NSTEPS=10000,visualize=False,save_best_weights=False):\n",
    "        epsiode = 1\n",
    "        \n",
    "        \n",
    "        # prepare agent\n",
    "        agent.agent_init(agent_parameters)\n",
    "        state,info= env.reset() \n",
    "        # choose initial action based on agent's results\n",
    "        action = agent.agent_start(state)\n",
    "        done = False\n",
    "        epsiode_reward = 0\n",
    "        episode_len = 0\n",
    "        \n",
    "        for i in tqdm(range(1,NSTEPS+1)):\n",
    "            self.step = i\n",
    "            state,reward,terminated,truncated,info=env.step(action)\n",
    "            epsiode_reward += reward\n",
    "            done = terminated or truncated\n",
    "            if(self.step%self.output_step==0):\n",
    "                self.summarize()\n",
    "                if(visualize):\n",
    "                    if(len(self.epsiode_rewards)>0):\n",
    "                        self.plot_live(self.epsiode_rewards)\n",
    "            if(done):\n",
    "                loss = agent.agent_end(reward)\n",
    "                self.loss.append(loss)\n",
    "                if(save_best_weights):\n",
    "                    self.create_model_dir()\n",
    "                    if(len(self.epsiode_rewards)==0):\n",
    "                        model_name = self.model_dir+f\"model_{self.step}\"\n",
    "                        self.utils.save_model(agent,model_name)\n",
    "                    else:\n",
    "                        if(epsiode_reward>max(self.epsiode_rewards)):\n",
    "                            model_name = self.model_dir+f\"model_{self.step}\"\n",
    "                            self.utils.save_model(agent,model_name)\n",
    "                self.epsiode_rewards.append(epsiode_reward)\n",
    "                self.episode_lens.append(episode_len)\n",
    "                epsiode += 1\n",
    "                self.num_episodes += 1\n",
    "                # restart next episode\n",
    "                state,_= env.reset() \n",
    "                action = agent.agent_start(state)\n",
    "                done = False\n",
    "                epsiode_reward = 0\n",
    "                episode_len = 0\n",
    "            else:\n",
    "                action = agent.agent_step(reward,state)\n",
    "                episode_len+=1\n",
    "        return agent\n",
    "\n",
    "    def evaluate(self,agent,env,n_episodes=10,seed=1,visualize=False,eval_espilon=0.001):\n",
    "        epsiode_rewards = []\n",
    "        for episode in range(1,n_episodes+1):\n",
    "            state,info = env.reset()\n",
    "            action = agent.greedy_policy(state,eval_espilon)\n",
    "            done = False\n",
    "            epsiode_reward = 0\n",
    "            episode_len = 0\n",
    "            while not done:\n",
    "                state,reward,terminated,truncated,info=env.step(action)\n",
    "                epsiode_reward += reward\n",
    "                done = terminated or truncated\n",
    "                action = agent.greedy_policy(state,eval_espilon)\n",
    "                episode_len += 1\n",
    "            epsiode_rewards.append(epsiode_reward)\n",
    "            self.episode_summarize(episode,epsiode_reward)\n",
    "            #if(visualize):\n",
    "            #    env.summarize()\n",
    "        mean_rew = np.average(epsiode_rewards)\n",
    "        std_rew = np.std(epsiode_rewards)\n",
    "        return (mean_rew,std_rew) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\") \n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def run():\n",
    "    env = gym.make(\"MountainCarContinuous-v0\") #TradingEnv(data_file=\"usdjpy.csv\",start_tick=100,end_tick=500,window_length=9,debug=False,pos=False,tech=False)\n",
    "    #test_env = TradingEnv(data_file=\"usdjpy.csv\",start_tick=10000,end_tick=11000,window_length=9,debug=False,pos=False,tech=False)\n",
    "    \n",
    "    \n",
    "    n_state = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.shape[0]\n",
    "\n",
    "    agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': n_state,\n",
    "        'num_hidden_units': 128,\n",
    "        'num_actions': n_actions,\n",
    "        \"network_type\":\"dqn\"\n",
    "    },\n",
    "    'replay_buffer_size': 1_000_000,\n",
    "    'minibatch_sz': 32,\n",
    "    'observation_size':n_state,\n",
    "    'num_replay_updates_per_step': 1,\n",
    "    \"step_size\": 3e-4,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon': 1,\n",
    "    'update_freq':500,\n",
    "    'warmup_steps':500,\n",
    "    'double_dqn':True\n",
    "    }\n",
    "    #agent = ActorCriticAgent() #DQNAgent()\n",
    "    agent = A2CAgent()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    rl = RL()\n",
    "    #rl.set_seed(1)\n",
    "    trained_agent = rl.learn(agent,env,agent_parameters,NSTEPS=50_000,visualize=True,save_best_weights=False)\n",
    "    mean,std=rl.evaluate(trained_agent,env,n_episodes=5,visualize=True)\n",
    "    print(f\"Mean reward: {mean}, Standard deviation: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m rl \u001b[38;5;241m=\u001b[39m RL()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#rl.set_seed(1)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m trained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43magent_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNSTEPS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43msave_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m mean,std\u001b[38;5;241m=\u001b[39mrl\u001b[38;5;241m.\u001b[39mevaluate(trained_agent,env,n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Standard deviation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 99\u001b[0m, in \u001b[0;36mRL.learn\u001b[0;34m(self, agent, env, agent_parameters, NSTEPS, visualize, save_best_weights)\u001b[0m\n\u001b[1;32m     97\u001b[0m state,info\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset() \n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# choose initial action based on agent's results\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    101\u001b[0m epsiode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 94\u001b[0m, in \u001b[0;36mA2CAgent.agent_start\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mI \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_state \u001b[38;5;241m=\u001b[39m state \u001b[38;5;66;03m#torch.tensor(np.array([state]),dtype=torch.float32,device=self.device)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action\n",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m, in \u001b[0;36mA2CAgent.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     60\u001b[0m policy,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic_network(state)\n\u001b[1;32m     61\u001b[0m action_probabilities \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 62\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction_probabilities\u001b[49m\u001b[43m)\u001b[49m, p\u001b[38;5;241m=\u001b[39maction_probabilities)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asmazaev/Projects/TensorAeroSpace/.venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "env = gym.make('CartPole-v0', render_mode=\"rgb_array_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from collections import namedtuple, deque \n",
    "\n",
    "##Importing the model (function approximator for Q-table)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  #replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns form environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        \n",
    "        #Q- Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=LR)\n",
    "        \n",
    "        # Replay memory \n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE,BATCH_SIZE,seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_step, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_step, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step+1)% UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get radom subset and learn\n",
    "\n",
    "            if len(self.memory)>BATCH_SIZE:\n",
    "                experience = self.memory.sample()\n",
    "                self.learn(experience, GAMMA)\n",
    "    def act(self, state, eps = 0):\n",
    "        \"\"\"Returns action for given state as per current policy\n",
    "        Params\n",
    "        =======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #Epsilon -greedy action selction\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        =======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_state, dones = experiences\n",
    "        ## TODO: compute and minimize the loss\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        # Local model is one which we need to train so it's in training mode\n",
    "        self.qnetwork_local.train()\n",
    "        # Target model is one with which we need to get our target so it's in evaluation mode\n",
    "        # So that when we do a forward pass with target model it does not calculate gradient.\n",
    "        # We will update target model weights with soft_update function\n",
    "        self.qnetwork_target.eval()\n",
    "        #shape of output from the model (batch_size,action_dim) = (64,4)\n",
    "        predicted_targets = self.qnetwork_local(states).gather(1,actions)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            labels_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # .detach() ->  Returns a new Tensor, detached from the current graph.\n",
    "        labels = rewards + (gamma* labels_next*(1-dones))\n",
    "        \n",
    "        loss = criterion(predicted_targets,labels).to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local,self.qnetwork_target,TAU)\n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        =======\n",
    "            local model (PyTorch model): weights will be copied from\n",
    "            target model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(),\n",
    "                                           local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
    "            \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed -size buffe to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experiences(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mobservation_space\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[0;32m---> 49\u001b[0m scores\u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#plot the scores\u001b[39;00m\n\u001b[1;32m     52\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     21\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t):\n\u001b[0;32m---> 23\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     next_state, reward, done, info, termenated  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     25\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstep(state,action,reward,next_state,done)\n",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns action for given state as per current policy\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Params\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    =======\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m        state (array_like): current state\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        eps (float): epsilon, for epsilon-greedy action selection\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got tuple)"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=8,action_size=4,seed=0)\n",
    "\n",
    "def dqn(n_episodes= 200, max_t = 1000, eps_start=1.0, eps_end = 0.01,\n",
    "       eps_decay=0.996):\n",
    "    \"\"\"Deep Q-Learning\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training epsiodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon \n",
    "        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n",
    "        \n",
    "    \"\"\"\n",
    "    scores = [] # list containing score from each episode\n",
    "    scores_window = deque(maxlen=100) # last 100 scores\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state,eps)\n",
    "            next_state, reward, done, info, termenated  = env.step(action)\n",
    "            agent.step(state,action,reward,next_state,done)\n",
    "            ## above step decides whether we will train(learn) the network\n",
    "            ## actor (local_qnetwork) or we will fill the replay buffer\n",
    "            ## if len replay buffer is equal to the batch size then we will\n",
    "            ## train the network or otherwise we will add experience tuple in our \n",
    "            ## replay buffer.\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            scores_window.append(score) ## save the most recent score\n",
    "            scores.append(score) ## sae the most recent score\n",
    "            eps = max(eps*eps_decay,eps_end)## decrease the epsilon\n",
    "            print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode,np.mean(scores_window)), end=\"\")\n",
    "            if i_episode %100==0:\n",
    "                print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode,np.mean(scores_window)))\n",
    "                \n",
    "            if np.mean(scores_window)>=200.0:\n",
    "                print('\\nEnvironment solve in {:d} epsiodes!\\tAverage score: {:.2f}'.format(i_episode-100,\n",
    "                                                                                           np.mean(scores_window)))\n",
    "                torch.save(agent.qnetwork_local.state_dict(),'checkpoint.pth')\n",
    "                break\n",
    "    return scores\n",
    "\n",
    "scores= dqn()\n",
    "\n",
    "#plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)),scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[0;32m---> 49\u001b[0m scores\u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#plot the scores\u001b[39;00m\n\u001b[1;32m     52\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     21\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t):\n\u001b[0;32m---> 23\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     next_state, reward, done, info, termenated \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     25\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstep(state,action,reward,next_state,done)\n",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns action for given state as per current policy\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Params\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    =======\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m        state (array_like): current state\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        eps (float): epsilon, for epsilon-greedy action selection\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got tuple)"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=8,action_size=4,seed=0)\n",
    "\n",
    "def dqn(n_episodes= 200, max_t = 1000, eps_start=1.0, eps_end = 0.01,\n",
    "       eps_decay=0.996):\n",
    "    \"\"\"Deep Q-Learning\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training epsiodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon \n",
    "        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n",
    "        \n",
    "    \"\"\"\n",
    "    scores = [] # list containing score from each episode\n",
    "    scores_window = deque(maxlen=100) # last 100 scores\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state,eps)\n",
    "            next_state, reward, done, info, termenated = env.step(action)\n",
    "            agent.step(state,action,reward,next_state,done)\n",
    "            ## above step decides whether we will train(learn) the network\n",
    "            ## actor (local_qnetwork) or we will fill the replay buffer\n",
    "            ## if len replay buffer is equal to the batch size then we will\n",
    "            ## train the network or otherwise we will add experience tuple in our \n",
    "            ## replay buffer.\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            scores_window.append(score) ## save the most recent score\n",
    "            scores.append(score) ## sae the most recent score\n",
    "            eps = max(eps*eps_decay,eps_end)## decrease the epsilon\n",
    "            print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode,np.mean(scores_window)), end=\"\")\n",
    "            if i_episode %100==0:\n",
    "                print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode,np.mean(scores_window)))\n",
    "                \n",
    "            if np.mean(scores_window)>=200.0:\n",
    "                print('\\nEnvironment solve in {:d} epsiodes!\\tAverage score: {:.2f}'.format(i_episode-100,\n",
    "                                                                                           np.mean(scores_window)))\n",
    "                torch.save(agent.qnetwork_local.state_dict(),'checkpoint.pth')\n",
    "                break\n",
    "    return scores\n",
    "\n",
    "scores= dqn()\n",
    "\n",
    "#plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)),scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
