{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "from threading import Thread\n",
    "from multiprocessing import cpu_count\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "GLOBAL_EP = 0\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(actor_lr)\n",
    "        self.entropy_beta = 0.01\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_size,))\n",
    "        dense_1 = Dense(hidden_size, activation='relu')(state_input)\n",
    "        dense_2 = Dense(hidden_size, activation='relu')(dense_1)\n",
    "        policy = Dense(self.action_size, activation='softmax')(dense_2)\n",
    "        return tf.keras.models.Model(state_input, policy)\n",
    "    \n",
    "    def compute_loss(self, actions, logits, advantages):\n",
    "        ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "        entropy_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = ce_loss(\n",
    "            actions, logits, sample_weight=tf.stop_gradient(advantages))\n",
    "        entropy = entropy_loss(logits, logits)\n",
    "        return policy_loss - self.entropy_beta * entropy\n",
    "\n",
    "    def train(self, states, actions, advantages):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            curr_P = self.model(states, training=True)\n",
    "            loss = self.compute_loss(actions, curr_P, advantages)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_size,)),\n",
    "            Dense(hidden_size, activation='relu'),\n",
    "            Dense(hidden_size, activation='relu'),\n",
    "            # Dense(16, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "class Worker(Thread):\n",
    "    def __init__(self, id, env, gamma, global_actor, global_critic):\n",
    "        Thread.__init__(self)\n",
    "        self.name = \"w%i\" % id\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "        \n",
    "        self.actor = Actor(self.state_size, self.action_size,\n",
    "                           )\n",
    "        self.critic = Critic(self.state_size)\n",
    "        \n",
    "        # sync local networks with global networks\n",
    "        self.sync_with_global()\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        probs = self.actor.model.predict(state)\n",
    "        return np.random.choice(self.action_size, p=probs[0])\n",
    "    \n",
    "    def n_step_td_target(self, rewards, next_Q, done):\n",
    "        td_targets = np.zeros_like(rewards)\n",
    "        R_to_go = 0\n",
    "        \n",
    "        if not done:\n",
    "            R_to_go = next_Q\n",
    "        \n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            R_to_go = rewards[k] + self.gamma * R_to_go \n",
    "            td_targets[k] = R_to_go\n",
    "        return td_targets\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "    \n",
    "    def sync_with_global(self):\n",
    "        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "    \n",
    "    def run(self):\n",
    "        global GLOBAL_EP\n",
    "        while max_episodes > GLOBAL_EP:\n",
    "        # for episode in range(max_episodes):\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            states     = []\n",
    "            actions    = []\n",
    "            rewards    = []\n",
    "            \n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                state      = np.reshape(state, [1, self.state_size])\n",
    "                action     = np.reshape(action, [1, 1])\n",
    "                reward     = np.reshape(reward, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                \n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                state = next_state[0]\n",
    "                episode_reward += reward[0][0]\n",
    "                \n",
    "                if len(states) >= update_interval or done:\n",
    "                    states  = self.list_to_batch(states)\n",
    "                    actions = self.list_to_batch(actions)\n",
    "                    rewards = self.list_to_batch(rewards)\n",
    "                    \n",
    "                    curr_Qs = self.critic.model.predict(states)\n",
    "                    next_Q = self.critic.model.predict(next_state)\n",
    "                    \n",
    "                    td_targets   = self.n_step_td_target(rewards, next_Q, done)\n",
    "                    # advantages   = td_targets - self.critic.model.predict(states)\n",
    "                    advantages   = td_targets - curr_Qs\n",
    "                    \n",
    "                    actor_loss = self.global_actor.train(states, actions, advantages)\n",
    "                    critic_loss = self.global_critic.train(states, td_targets)\n",
    "\n",
    "                    self.sync_with_global()\n",
    "                    states     = []\n",
    "                    actions    = []\n",
    "                    rewards    = []\n",
    "\n",
    "            print(self.name + ' | EP{} EpisodeReward={}'.format(GLOBAL_EP+1, episode_reward))\n",
    "            GLOBAL_EP += 1\n",
    "\n",
    "class A3CAgent:\n",
    "    \n",
    "    def __init__(self, env_name, gamma):\n",
    "        env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.gamma = gamma\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "        self.global_actor = Actor(self.state_size, self.action_size,\n",
    "                                 )\n",
    "        self.global_critic = Critic(self.state_size)\n",
    "        \n",
    "        self.num_workers = cpu_count()\n",
    "        \n",
    "    def train(self):\n",
    "        print(\"Training on {} cores\".format(self.num_workers))\n",
    "        input(\"Enter to start\")\n",
    "        self.workers = []\n",
    "\n",
    "        for i in range(self.num_workers):\n",
    "            env = gym.make(self.env_name)\n",
    "            self.workers.append(Worker(\n",
    "                i, env, self.gamma, self.global_actor, self.global_critic))\n",
    "        \n",
    "        # [worker.start() for worker in self.workers]\n",
    "        # [worker.join() for worker in self.workers]\n",
    "        \n",
    "        for worker in self.workers:\n",
    "            worker.start()\n",
    "\n",
    "        for worker in self.workers:\n",
    "            worker.join()\n",
    "    \n",
    "    # def save_model(self):\n",
    "    #     self.global_critic.save(\"a3c_value_model.h5\")\n",
    "    #     self.global_actor.save(\"a3c_policy_model.h5\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env_name = \"CartPole-v0\"\n",
    "    # set environment\n",
    "    actor_lr = 0.0005\n",
    "    critic_lr = 0.001\n",
    "    gamma = 0.99\n",
    "    hidden_size = 128\n",
    "    update_interval = 50\n",
    "    \n",
    "    max_episodes = 500  # Set total number of episodes to train agent on.\n",
    "    agent = A3CAgent(env_name, gamma)\n",
    "    agent.train()\n",
    "    # agent.save_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
