{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acba9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.losses as kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e03965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensoraerospace.envs.unity_env import get_plane_env, unity_discrete_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1d31fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "        self.v = tf.keras.layers.Dense(1, activation = None)\n",
    "\n",
    "    def call(self, input_data):\n",
    "        x = self.d1(input_data)\n",
    "        v = self.v(x)\n",
    "        return v\n",
    "    \n",
    "\n",
    "class actor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "        self.a = tf.keras.layers.Dense(3 ** 7,activation='softmax')\n",
    "        self.r = tf.keras.layers.Dense(1, activation='relu')\n",
    "\n",
    "    def call(self, input_data, return_reward=False):\n",
    "        x = self.d1(input_data)\n",
    "        a = self.a(x)\n",
    "        if return_reward:\n",
    "            r = self.r(x)\n",
    "            return a, r\n",
    "        return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0081d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, gamma = 0.99):\n",
    "        self.gamma = gamma\n",
    "        # self.a_opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "        # self.c_opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
    "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
    "        self.actor = actor()\n",
    "        self.critic = critic()\n",
    "        self.clip_pram = 0.2\n",
    "\n",
    "          \n",
    "    def act(self,state):\n",
    "        prob = self.actor(np.array([state]))\n",
    "        prob = prob.numpy()\n",
    "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "        action = dist.sample()\n",
    "        if int(action.numpy()[0]) >= 3 ** 7:\n",
    "            return int(action.numpy()[0]) - 1\n",
    "        return int(action.numpy()[0])\n",
    "  \n",
    "\n",
    "\n",
    "    def actor_loss(self, probs, actions, adv, old_probs, closs):\n",
    "        \n",
    "        probability = probs      \n",
    "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability,tf.math.log(probability))))\n",
    "        #print(probability)\n",
    "        #print(entropy)\n",
    "        sur1 = []\n",
    "        sur2 = []\n",
    "        \n",
    "        for pb, t, op,a  in zip(probability, adv, old_probs, actions):\n",
    "            t =  tf.constant(t)\n",
    "            #print(a)\n",
    "            #op =  tf.constant(op)\n",
    "            #print(f\"t{t}\")\n",
    "            #ratio = tf.math.exp(tf.math.log(pb + 1e-10) - tf.math.log(op + 1e-10))\n",
    "            ratio = tf.math.divide(pb[a],op[a])\n",
    "            #print(f\"ratio{ratio}\")\n",
    "            s1 = tf.math.multiply(ratio,t)\n",
    "            #print(f\"s1{s1}\")\n",
    "            s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram),t)\n",
    "            #print(f\"s2{s2}\")\n",
    "            sur1.append(s1)\n",
    "            sur2.append(s2)\n",
    "\n",
    "        sr1 = tf.stack(sur1)\n",
    "        sr2 = tf.stack(sur2)\n",
    "        \n",
    "        #closs = tf.reduce_mean(tf.math.square(td))\n",
    "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "    \n",
    "    def auxillary_task(self, r, rewards):\n",
    "        loss = tf.reduce_mean(tf.math.square(r - rewards))\n",
    "        return loss\n",
    "\n",
    "    def learn(self, states, actions,  adv , old_probs, discnt_rewards, rewards):\n",
    "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
    "        adv = tf.reshape(adv, (len(adv),))\n",
    "\n",
    "        old_p = old_probs\n",
    "\n",
    "        old_p = tf.reshape(old_p, (len(old_p),3 ** 7))\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            p, r = self.actor(states, return_reward=True, training=True)\n",
    "            v =  self.critic(states,training=True)\n",
    "            v = tf.reshape(v, (len(v),))\n",
    "            td = tf.math.subtract(discnt_rewards, v)\n",
    "            c_loss = 0.5 * kls.mean_squared_error(discnt_rewards, v)\n",
    "            a_loss = self.actor_loss(p, actions, adv, old_probs, c_loss) + self.auxillary_task(r, rewards)\n",
    "            \n",
    "        grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n",
    "        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
    "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
    "        return a_loss, c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "74d059db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward(env):\n",
    "    #print(\"yes\")\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    for i in range(10):\n",
    "        action = np.argmax(agentoo7.actor(np.array([state])).numpy())\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        done = True\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9ee06ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.2.1-exp.1 and communication version 1.5.0\n",
      "[INFO] Connected new brain: My Behavior?team=0\n",
      "[WARNING] uint8_visual was set to true, but visual observations are not in use. This setting will not have any effect.\n",
      "[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n",
      "new episod\n",
      "total test reward is -10.0\n"
     ]
    }
   ],
   "source": [
    "env= unity_discrete_env(env_path=\"C:\\\\Users\\\\DexFrost89\\\\3_step\\\\builds\\\\My project.exe\")\n",
    "def preprocess1(states, actions, rewards, done, values, gamma):\n",
    "    g = 0\n",
    "    lmbda = 0.95\n",
    "    returns = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * done[i] - values[i]\n",
    "        g = delta + gamma * lmbda * dones[i] * g\n",
    "        returns.append(g + values[i])\n",
    "\n",
    "    returns.reverse()\n",
    "    adv = np.array(returns, dtype=np.float32) - values[:-1]\n",
    "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    returns = np.array(returns, dtype=np.float32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    return states, actions, returns, adv, rewards    \n",
    "\n",
    "\n",
    "tf.random.set_seed(336699)\n",
    "agentoo7 = agent()\n",
    "steps = 10\n",
    "ep_reward = []\n",
    "total_avgr = []\n",
    "target = False \n",
    "best_reward = 0\n",
    "avg_rewards_list = []\n",
    "\n",
    "\n",
    "for s in range(steps):\n",
    "    if target == True:\n",
    "        break\n",
    "  \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    all_aloss = []\n",
    "    all_closs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    probs = []\n",
    "    dones = []\n",
    "    values = []\n",
    "    print(\"new episod\")\n",
    "\n",
    "    for e in range(10):\n",
    "   \n",
    "        action = agentoo7.act(state)\n",
    "        value = agentoo7.critic(np.array([state])).numpy()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dones.append(1-done)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        #actions.append(tf.one_hot(action, 2, dtype=tf.int32).numpy().tolist())\n",
    "        actions.append(action)\n",
    "        prob = agentoo7.actor(np.array([state]))\n",
    "        probs.append(prob[0])\n",
    "        values.append(value[0][0])\n",
    "        state = next_state\n",
    "        if done:\n",
    "      env.reset()\n",
    "  \n",
    "  value = agentoo7.critic(np.array([state])).numpy()\n",
    "  values.append(value[0][0])\n",
    "  np.reshape(probs, (len(probs),3 ** 7))\n",
    "  probs = np.stack(probs, axis=0)\n",
    "\n",
    "  states, actions,returns, adv, rewards  = preprocess1(states, actions, rewards, dones, values, 1)\n",
    "\n",
    "  for epocs in range(1):\n",
    "      al,cl = agentoo7.learn(states, actions, adv, probs, returns, rewards)\n",
    "      # print(f\"al{al}\") \n",
    "      # print(f\"cl{cl}\")   \n",
    "\n",
    "  avg_reward = np.mean([test_reward(env) for _ in range(5)])\n",
    "  print(f\"total test reward is {avg_reward}\")\n",
    "  avg_rewards_list.append(avg_reward)\n",
    "  if avg_reward > best_reward:\n",
    "        print('best reward=' + str(avg_reward))\n",
    "        agentoo7.actor.save('model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "        agentoo7.critic.save('model_critic_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "        best_reward = avg_reward\n",
    "  if best_reward == 200:\n",
    "        target = True\n",
    "  env.reset()\n",
    "\n",
    "env.close()\n",
    "    \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
